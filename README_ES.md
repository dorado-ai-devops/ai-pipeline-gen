# âš™ï¸ ai-pipeline-gen

> Microservicio generador de pipelines CI/CD a partir de descripciones textuales, impulsado por modelos de lenguaje como GPT-4o (OpenAI) u Ollama (LLaMA3, Phi-3, etc). Recibe una descripciÃ³n de alto nivel y devuelve un pipeline YAML funcional en formato Jenkins declarativo.

---

## ğŸš€ Funcionalidades

- ğŸ› ï¸ Genera pipelines CI/CD a partir de texto natural
- ğŸ§  Soporta OpenAI GPT-4o y modelos locales vÃ­a Ollama
- ğŸ”Œ Expone microservicio Flask con endpoint `/generate`
- ğŸ“¦ CLI compatible para ejecuciÃ³n local
- ğŸ“„ Prompts modulares editables (`pipeline_gen.prompt`)
- ğŸ³ Contenedor listo para despliegue en Kubernetes
- ğŸ” Integrable con `ai-gateway` como backend IA centralizado

---

## ğŸ“¦ Estructura del Proyecto


```
ai-pipeline-gen/
â”œâ”€â”€ app.py                  # Microservicio Flask (API /generate)
â”œâ”€â”€ lib/                    # Clientes IA + lÃ³gica principal
â”‚   â”œâ”€â”€ ollama_client.py
â”‚   â”œâ”€â”€ openai_client.py
â”‚   â”œâ”€â”€ pipeline_gen.py     # Punto de entrada CLI
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ pipeline_gen.prompt # Plantilla de prompt para generaciÃ³n
â”œâ”€â”€ run.sh                  # Lanzador simple
â”œâ”€â”€ Makefile                # Build/despliegue automatizado
â”œâ”€â”€ Dockerfile              # Imagen del microservicio
â”œâ”€â”€ requirements.txt        # Dependencias Python
â””â”€â”€ README.md               # DocumentaciÃ³n del proyecto
```

---

## ğŸ” DescripciÃ³n de Componentes

### `app.py`

Microservicio Flask que expone `/generate`.  
- Entrada esperada:
```json
{
  "description": "Pipeline bÃ¡sico con test y deploy",
  "mode": "ollama" // o "openai"
}
```
- Devuelve un YAML generado en respuesta.

### `lib/pipeline_gen.py`

Entrada CLI para uso local.  
Acepta archivo `.txt` como input con la descripciÃ³n del pipeline.  
Ejemplo:
```bash
python3 lib/pipeline_gen.py --input mydesc.txt --mode openai
```

### `lib/openai_client.py` y `lib/ollama_client.py`

- Clientes independientes para cada backend.
- Soportan inferencia vÃ­a API externa (`openai`) o local (`ollama`).
- Se seleccionan desde la CLI o el payload JSON.

### `prompts/pipeline_gen.prompt`

Prompt dinÃ¡mico con la variable `{{description}}` que serÃ¡ sustituida por la entrada del usuario.

---

## ğŸ³ Docker

```bash
docker build -t ai-pipeline-gen:dev .
docker run -p 5003:5003 ai-pipeline-gen:dev
```

---

## ğŸ› ï¸ Makefile

Tareas Ãºtiles disponibles:

```bash
make build              # Construye la imagen
make load               # Carga en KIND local
make update-values      # Refresca values.yaml (Helm)
make sync               # Sincroniza con ArgoCD
make release VERSION=vX.Y.Z  # Build completo + despliegue
```

---

## ğŸŒ IntegraciÃ³n con ai-gateway

Se espera que el servicio estÃ© accesible internamente en Kubernetes en la siguiente direcciÃ³n:

```
http://pipeline-gen-service.devops-ai.svc.cluster.local:80/generate
```

Y el `ai-gateway` reenvÃ­a peticiones como:

```json
{
  "description": "Pipeline de build y despliegue con validaciones Helm",
  "mode": "ollama"
}
```

---

## ğŸ§ª Ejemplo de salida

```groovy
pipeline {
  agent any
  stages {
    stage('Build') {
      steps {
        sh 'make build'
      }
    }
    stage('Test') {
      steps {
        sh 'pytest tests/'
      }
    }
    stage('Deploy') {
      steps {
        sh './deploy.sh'
      }
    }
  }
}
```

---

## ğŸ§  Inspirado por

- [OpenAI API](https://platform.openai.com)
- [Ollama](https://ollama.com)
- [Jenkins Pipelines](https://www.jenkins.io/doc/book/pipeline/)

---

## ğŸ‘¨â€ğŸ’» Autor

- **Dani** â€” [@dorado-ai-devops](https://github.com/dorado-ai-devops)

---

## ğŸ›¡ï¸ Licencia

Licencia PÃºblica General GNU v3.0
