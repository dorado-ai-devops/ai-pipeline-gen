# âš™ï¸ ai-pipeline-gen

> CI/CD pipeline generator microservice powered by language models like GPT-4o (OpenAI) or Ollama (LLaMA3, Phi-3, etc). It receives a high-level description and returns a functional declarative Jenkins YAML pipeline.

---

## ğŸš€ Features

- ğŸ› ï¸ Generates CI/CD pipelines from natural language
- ğŸ§  Supports OpenAI GPT-4o and local models via Ollama
- ğŸ”Œ Flask microservice with `/generate` endpoint
- ğŸ“¦ CLI compatible for local execution
- ğŸ“„ Editable modular prompts (`pipeline_gen.prompt`)
- ğŸ³ Docker-ready for Kubernetes deployment
- ğŸ” Integrates with `ai-gateway` as centralized AI backend

---

## ğŸ“¦ Project Structure

```
ai-pipeline-gen/
â”œâ”€â”€ app.py                  # Flask microservice (API /generate)
â”œâ”€â”€ lib/                    # AI clients + core logic
â”‚   â”œâ”€â”€ ollama_client.py
â”‚   â”œâ”€â”€ openai_client.py
â”‚   â”œâ”€â”€ pipeline_gen.py     # CLI entry point
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ pipeline_gen.prompt # Prompt template for generation
â”œâ”€â”€ run.sh                  # Simple launcher
â”œâ”€â”€ Makefile                # Automated build/deploy
â”œâ”€â”€ Dockerfile              # Microservice image
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md               # Project documentation
```

---

## ğŸ” Component Breakdown

### `app.py`

Flask microservice exposing `/generate`.

- Expected input:
```json
{
  "description": "Basic pipeline with test and deploy",
  "mode": "ollama" // or "openai"
}
```
- Returns generated Jenkins YAML.

### `lib/pipeline_gen.py`

CLI entry point for local usage.  
Accepts `.txt` file as input with the pipeline description.

Example:
```bash
python3 lib/pipeline_gen.py --input mydesc.txt --mode openai
```

### `lib/openai_client.py` and `lib/ollama_client.py`

- Dedicated clients for each backend.
- Support inference via remote (`openai`) or local (`ollama`) API.
- Mode is selected via CLI or JSON payload.

### `prompts/pipeline_gen.prompt`

Dynamic prompt template with `{{description}}` placeholder for user input.

---

## ğŸ³ Docker

```bash
docker build -t ai-pipeline-gen:dev .
docker run -p 5003:5003 ai-pipeline-gen:dev
```

---

## ğŸ› ï¸ Makefile

Available tasks:

```bash
make build              # Build Docker image
make load               # Load into local KIND cluster
make update-values      # Refresh values.yaml (Helm)
make sync               # Sync with ArgoCD
make release VERSION=vX.Y.Z  # Full release pipeline
```

---

## ğŸŒ ai-gateway Integration

Service is expected to be available at:

```
http://pipeline-gen-service.devops-ai.svc.cluster.local:80/generate
```

And the `ai-gateway` forwards requests like:

```json
{
  "description": "Build and deploy pipeline with Helm validations",
  "mode": "ollama"
}
```

---

## ğŸ§ª Sample Output

```groovy
pipeline {
  agent any
  stages {
    stage('Build') {
      steps {
        sh 'make build'
      }
    }
    stage('Test') {
      steps {
        sh 'pytest tests/'
      }
    }
    stage('Deploy') {
      steps {
        sh './deploy.sh'
      }
    }
  }
}
```

---

## ğŸ§  Inspired by

- [OpenAI API](https://platform.openai.com)
- [Ollama](https://ollama.com)
- [Jenkins Pipelines](https://www.jenkins.io/doc/book/pipeline/)

---

## ğŸ‘¨â€ğŸ’» Author

- **Dani** â€” [@dorado-ai-devops](https://github.com/dorado-ai-devops)

---

## ğŸ›¡ï¸ License

GNU General Public License v3.0